import time
import uuid
import json
import csv
import random
import numpy as np
import psutil
import threading
from concurrent.futures import ThreadPoolExecutor, TimeoutError
from pymongo import MongoClient
from arango import ArangoClient
from couchbase.cluster import Cluster
from couchbase.options import ClusterOptions
from couchbase.auth import PasswordAuthenticator
from couchbase.management.buckets import CreateBucketSettings
from couchbase.exceptions import BucketAlreadyExistsException
import argparse

# –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è
THREADS = 10
TIMEOUT = 120  # –¢–∞–π–º–∞—É—Ç –¥–ª—è –æ–ø–µ—Ä–∞—Ü—ñ–π –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
PAUSE_BETWEEN_EXPERIMENTS = 30  # –ü–∞—É–∑–∞ –º—ñ–∂ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
TEST_DOC = {
    "name": "Test",
    "value": 123,
    "uuid": None,
    "data": None  # –ë—É–¥–µ –∑–∞–ø–æ–≤–Ω–µ–Ω–æ –¥–∞–Ω–∏–º–∏ —Ä—ñ–∑–Ω–æ–≥–æ —Ä–æ–∑–º—ñ—Ä—É
}

# –°—Ü–µ–Ω–∞—Ä—ñ—ó –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
WORKLOAD_SCENARIOS = {
    "read_heavy": {"read": 90, "write": 10, "description": "90% —á–∏—Ç–∞–Ω–Ω—è, 10% –∑–∞–ø–∏—Å—É"},
    "balanced": {"read": 50, "write": 50, "description": "50% —á–∏—Ç–∞–Ω–Ω—è, 50% –∑–∞–ø–∏—Å—É"},
    "write_heavy": {"read": 10, "write": 90, "description": "10% —á–∏—Ç–∞–Ω–Ω—è, 90% –∑–∞–ø–∏—Å—É"},
    "read_only": {"read": 100, "write": 0, "description": "100% —á–∏—Ç–∞–Ω–Ω—è"},
    "write_only": {"read": 0, "write": 100, "description": "100% –∑–∞–ø–∏—Å—É"},
    "batch_write": {"read": 0, "write": 100, "description": "–ü–∞–∫–µ—Ç–Ω–∏–π –∑–∞–ø–∏—Å"},
    "complex_query": {"read": 100, "write": 0, "description": "–°–∫–ª–∞–¥–Ω—ñ –∑–∞–ø–∏—Ç–∏"}
}

# –†–æ–∑–º—ñ—Ä–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ (–≤ –ö–ë)
DOCUMENT_SIZES = {
    "small": {"size": 1, "description": "1KB"},
    "medium": {"size": 10, "description": "10KB"},
    "large": {"size": 100, "description": "100KB"},
    "xlarge": {"size": 1000, "description": "1MB"}
}

# –î–æ—Å—Ç—É–ø–Ω—ñ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö
AVAILABLE_DATABASES = ["mongodb", "arangodb", "couchbase"]

def get_db_connection(db_name):
    """–û—Ç—Ä–∏–º–∞–Ω–Ω—è –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö"""
    if db_name == "mongodb":
        print("üîå –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ MongoDB...")
        client = MongoClient("mongodb://localhost:27017/")
        db = client.benchmark
        collection = db.test
        collection.delete_many({})  # –û—á–∏—â–µ–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó
        
        return {
            "insert_fn": lambda doc: collection.insert_one(doc),
            "read_fn": lambda: list(collection.find({"name": "Test"}))
        }
        
    elif db_name == "arangodb":
        print("üîå –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ ArangoDB...")
        client = ArangoClient()
        db = client.db('_system', username='root', password='admin')
        
        if not db.has_database('benchmark'):
            db.create_database('benchmark')
        db = client.db('benchmark', username='root', password='admin')
        
        if db.has_collection('test'):
            db.delete_collection('test')
        col = db.create_collection('test')
        
        return {
            "insert_fn": lambda doc: col.insert(doc),
            "read_fn": lambda: list(col.find({"name": "Test"}))
        }
        
    elif db_name == "couchbase":
        print("üîå –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ Couchbase...")
        cluster = Cluster("couchbase://localhost", ClusterOptions(
            PasswordAuthenticator("admin", "admin123")))
        bucket_name = "benchmark"
        try:
            cluster.buckets().create_bucket(CreateBucketSettings(name=bucket_name, ram_quota_mb=100))
        except BucketAlreadyExistsException:
            pass
        
        bucket = cluster.bucket(bucket_name)
        collection = bucket.default_collection()
        
        inserted_keys = []
        
        def insert(doc):
            key = str(uuid.uuid4())
            collection.upsert(key, doc)
            inserted_keys.append(key)
        
        def read():
            if inserted_keys:
                key = random.choice(inserted_keys)
                try:
                    collection.get(key)
                except Exception:
                    pass
        
        return {
            "insert_fn": insert,
            "read_fn": read
        }
    else:
        raise ValueError(f"–ù–µ–ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞–Ω–∞ –±–∞–∑–∞ –¥–∞–Ω–∏—Ö: {db_name}")

class SystemMetrics:
    """–ö–ª–∞—Å –¥–ª—è –∑–±–æ—Ä—É —Å–∏—Å—Ç–µ–º–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫"""
    def __init__(self):
        self.metrics = []
        self.running = False
        self.thread = None

    def start(self):
        """–ó–∞–ø—É—Å–∫ –∑–±–æ—Ä—É –º–µ—Ç—Ä–∏–∫"""
        self.running = True
        self.thread = threading.Thread(target=self._collect_metrics)
        self.thread.start()

    def stop(self):
        """–ó—É–ø–∏–Ω–∫–∞ –∑–±–æ—Ä—É –º–µ—Ç—Ä–∏–∫"""
        self.running = False
        if self.thread:
            self.thread.join()

    def _collect_metrics(self):
        """–ó–±—ñ—Ä —Å–∏—Å—Ç–µ–º–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫"""
        while self.running:
            metrics = {
                "timestamp": time.time(),
                "cpu_percent": psutil.cpu_percent(),
                "memory_percent": psutil.virtual_memory().percent,
                "disk_io": psutil.disk_io_counters(),
                "net_io": psutil.net_io_counters()
            }
            self.metrics.append(metrics)
            time.sleep(1)

    def get_average_metrics(self):
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å–µ—Ä–µ–¥–Ω—ñ—Ö –∑–Ω–∞—á–µ–Ω—å –º–µ—Ç—Ä–∏–∫"""
        if not self.metrics:
            return None

        return {
            "avg_cpu": np.mean([m["cpu_percent"] for m in self.metrics]),
            "avg_memory": np.mean([m["memory_percent"] for m in self.metrics]),
            "avg_disk_read": np.mean([m["disk_io"].read_bytes for m in self.metrics]),
            "avg_disk_write": np.mean([m["disk_io"].write_bytes for m in self.metrics]),
            "avg_net_sent": np.mean([m["net_io"].bytes_sent for m in self.metrics]),
            "avg_net_recv": np.mean([m["net_io"].bytes_recv for m in self.metrics])
        }

def create_test_doc(size_kb):
    """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∑–∞–¥–∞–Ω–æ–≥–æ —Ä–æ–∑–º—ñ—Ä—É"""
    doc = TEST_DOC.copy()
    doc["uuid"] = str(uuid.uuid4())
    # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö –∑–∞–¥–∞–Ω–æ–≥–æ —Ä–æ–∑–º—ñ—Ä—É
    doc["data"] = "x" * (size_kb * 1024)  # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –ö–ë –≤ –±–∞–π—Ç–∏
    return doc

def generate_document_sizes(max_docs):
    """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –º–∞—Å–∏–≤—É —Ä–æ–∑–º—ñ—Ä—ñ–≤ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –∑ 10 –∫—Ä–æ–∫–∞–º–∏"""
    return np.geomspace(1000, max_docs, 10, dtype=int)

def run_benchmark(db_name, scenario_name, max_docs, doc_size):
    """–ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫—É"""
    print(f"\nüöÄ –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫—É –¥–ª—è {db_name}")
    print(f"üìä –°—Ü–µ–Ω–∞—Ä—ñ–π: {scenario_name}")
    print(f"üì¶ –†–æ–∑–º—ñ—Ä –¥–æ–∫—É–º–µ–Ω—Ç—É: {doc_size['description']}")
    
    scenario = WORKLOAD_SCENARIOS[scenario_name]
    results = []
    
    # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ä–æ–∑–º—ñ—Ä—ñ–≤ –Ω–∞–±–æ—Ä—ñ–≤ –¥–∞–Ω–∏—Ö
    doc_sizes = generate_document_sizes(max_docs)
    print(f"\nüìà –ë—É–¥–µ –ø—Ä–æ—Ç–µ—Å—Ç–æ–≤–∞–Ω–æ –Ω–∞—Å—Ç—É–ø–Ω—ñ —Ä–æ–∑–º—ñ—Ä–∏ –Ω–∞–±–æ—Ä—ñ–≤: {doc_sizes}")
    
    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Å–∏—Å—Ç–µ–º–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫
    system_metrics = SystemMetrics()
    
    # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ –ë–î
    db_connection = get_db_connection(db_name)
    insert_fn = lambda doc: db_connection["insert_fn"](doc)
    read_fn = db_connection["read_fn"]
    
    # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ä–æ–∑–º—ñ—Ä—É –Ω–∞–±–æ—Ä—É
    for num_docs in doc_sizes:
        print(f"\nüìä –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –∑ {num_docs} –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏...")
        
        # –ó–∞–ø—É—Å–∫ –∑–±–æ—Ä—É –º–µ—Ç—Ä–∏–∫
        system_metrics.start()
        
        # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤
        docs = [create_test_doc(doc_size["size"]) for _ in range(num_docs)]
        
        # –í–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è —á–∞—Å—É
        start_time = time.time()
        timeout_occurred = False
        
        try:
            with ThreadPoolExecutor(max_workers=THREADS) as executor:
                if scenario_name == "batch_write":
                    # –ü–∞–∫–µ—Ç–Ω–∏–π –∑–∞–ø–∏—Å
                    futures = [executor.submit(insert_fn, doc) for doc in docs]
                    for future in futures:
                        future.result(timeout=TIMEOUT)
                elif scenario_name == "complex_query":
                    # –°–∫–ª–∞–¥–Ω—ñ –∑–∞–ø–∏—Ç–∏
                    futures = [executor.submit(read_fn) for _ in range(num_docs)]
                    for future in futures:
                        future.result(timeout=TIMEOUT)
                else:
                    # –ó–≤–∏—á–∞–π–Ω–µ –∑–º—ñ—à–∞–Ω–µ –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
                    read_ops = int(num_docs * (scenario["read"] / 100))
                    write_ops = num_docs - read_ops
                    
                    ops = []
                    for _ in range(write_ops):
                        doc = create_test_doc(doc_size["size"])
                        ops.append(("write", doc))
                    
                    for _ in range(read_ops):
                        ops.append(("read", None))
                    
                    random.shuffle(ops)
                    
                    futures = []
                    for op_type, doc in ops:
                        if op_type == "write":
                            futures.append(executor.submit(insert_fn, doc))
                        else:
                            futures.append(executor.submit(read_fn))
                    
                    for future in futures:
                        future.result(timeout=TIMEOUT)
        
        except TimeoutError:
            print(f"‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –≤–∏–∫–æ–Ω–∞–Ω–Ω—ñ –æ–ø–µ—Ä–∞—Ü—ñ–π –∑ {num_docs} –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏")
            timeout_occurred = True
        
        total_time = time.time() - start_time
        
        # –ó—É–ø–∏–Ω–∫–∞ –∑–±–æ—Ä—É –º–µ—Ç—Ä–∏–∫
        system_metrics.stop()
        avg_metrics = system_metrics.get_average_metrics()
        
        # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –º–µ—Ç—Ä–∏–∫
        if timeout_occurred:
            throughput = 0
            avg_latency = TIMEOUT
        else:
            throughput = num_docs / total_time
            avg_latency = total_time / num_docs
        
        results.append({
            "database": db_name,
            "scenario": scenario_name,
            "document_size": doc_size["description"],
            "documents": num_docs,
            "total_time": total_time,
            "throughput": throughput,
            "avg_latency": avg_latency,
            "read_percentage": scenario["read"],
            "write_percentage": scenario["write"],
            "avg_cpu": avg_metrics["avg_cpu"],
            "avg_memory": avg_metrics["avg_memory"],
            "avg_disk_read": avg_metrics["avg_disk_read"],
            "avg_disk_write": avg_metrics["avg_disk_write"],
            "avg_net_sent": avg_metrics["avg_net_sent"],
            "avg_net_recv": avg_metrics["avg_net_recv"],
            "timeout_occurred": timeout_occurred
        })
        
        # –ü–∞—É–∑–∞ –º—ñ–∂ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏
        if num_docs != doc_sizes[-1]:  # –ù–µ —á–µ–∫–∞—î–º–æ –ø—ñ—Å–ª—è –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É
            print(f"‚è≥ –û—á—ñ–∫—É–≤–∞–Ω–Ω—è {PAUSE_BETWEEN_EXPERIMENTS} —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ –Ω–∞—Å—Ç—É–ø–Ω–∏–º –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–º...")
            time.sleep(PAUSE_BETWEEN_EXPERIMENTS)
    
    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    filename = f"benchmark_{db_name}_{scenario_name}_{doc_size['size']}kb.csv"
    with open(filename, mode="w", newline='', encoding='utf-8') as file:
        writer = csv.DictWriter(file, fieldnames=results[0].keys())
        writer.writeheader()
        writer.writerows(results)
    print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É —Ñ–∞–π–ª {filename}")
    
    return results

def run_all_benchmarks():
    """–ó–∞–ø—É—Å–∫ –≤—Å—ñ—Ö –±–µ–Ω—á–º–∞—Ä–∫—ñ–≤ –¥–ª—è –≤—Å—ñ—Ö –±–∞–∑ –¥–∞–Ω–∏—Ö"""
    all_results = []
    
    for db_name in AVAILABLE_DATABASES:
        print(f"\nüîç –ü–æ—á–∞—Ç–æ–∫ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –±–∞–∑–∏ –¥–∞–Ω–∏—Ö: {db_name}")
        
        for scenario_name in WORKLOAD_SCENARIOS.keys():
            for doc_size_name, doc_size in DOCUMENT_SIZES.items():
                try:
                    results = run_benchmark(db_name, scenario_name, 5000, doc_size)
                    all_results.extend(results)
                except Exception as e:
                    print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—ñ {db_name} –∑ —Å—Ü–µ–Ω–∞—Ä—ñ—î–º {scenario_name}: {str(e)}")
                    continue
        
        print(f"\n‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –±–∞–∑–∏ –¥–∞–Ω–∏—Ö: {db_name}")
        print(f"‚è≥ –û—á—ñ–∫—É–≤–∞–Ω–Ω—è {PAUSE_BETWEEN_EXPERIMENTS} —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ –Ω–∞—Å—Ç—É–ø–Ω–æ—é –±–∞–∑–æ—é –¥–∞–Ω–∏—Ö...")
        time.sleep(PAUSE_BETWEEN_EXPERIMENTS)
    
    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤—Å—ñ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª
    all_results_filename = "benchmark_all_results.csv"
    with open(all_results_filename, mode="w", newline='', encoding='utf-8') as file:
        writer = csv.DictWriter(file, fieldnames=all_results[0].keys())
        writer.writeheader()
        writer.writerows(all_results)
    print(f"\n‚úÖ –í—Å—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É —Ñ–∞–π–ª {all_results_filename}")

def main():
    parser = argparse.ArgumentParser(description='–ö–æ–º–ø–ª–µ–∫—Å–Ω–∏–π —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –±–µ–Ω—á–º–∞—Ä–∫—É NoSQL –±–∞–∑ –¥–∞–Ω–∏—Ö')
    parser.add_argument('--mode', choices=['single', 'all'], default='all',
                      help='–†–µ–∂–∏–º —Ä–æ–±–æ—Ç–∏: single - –æ–¥–∏–Ω —Ç–µ—Å—Ç, all - –≤—Å—ñ —Ç–µ—Å—Ç–∏')
    parser.add_argument('--db', choices=AVAILABLE_DATABASES,
                      help='–ë–∞–∑–∞ –¥–∞–Ω–∏—Ö –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è (—Ç—ñ–ª—å–∫–∏ –¥–ª—è —Ä–µ–∂–∏–º—É single)')
    parser.add_argument('--scenario', choices=WORKLOAD_SCENARIOS.keys(),
                      help='–°—Ü–µ–Ω–∞—Ä—ñ–π –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è (—Ç—ñ–ª—å–∫–∏ –¥–ª—è —Ä–µ–∂–∏–º—É single)')
    parser.add_argument('--max-docs', type=int, default=5000,
                      help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è')
    parser.add_argument('--doc-size', choices=DOCUMENT_SIZES.keys(), default='small',
                      help='–†–æ–∑–º—ñ—Ä —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤')
    
    args = parser.parse_args()
    
    if args.mode == 'all':
        run_all_benchmarks()
    else:
        if not args.db or not args.scenario:
            parser.error("–î–ª—è —Ä–µ–∂–∏–º—É single –ø–æ—Ç—Ä—ñ–±–Ω–æ –≤–∫–∞–∑–∞—Ç–∏ --db —Ç–∞ --scenario")
        run_benchmark(args.db, args.scenario, args.max_docs, DOCUMENT_SIZES[args.doc_size])

if __name__ == "__main__":
    main() 